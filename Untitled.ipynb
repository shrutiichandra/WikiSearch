{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'enwiki-latest-pages-articles26.xml-p42567204p42663461'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import search, match, findall, sub, compile, finditer, DOTALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "new_stopwords = [',', ':']\n",
    "stop_words = stop_words.union(new_stopwords)\n",
    "def remove_stop_words(word_tokens):\n",
    "    global stop_words\n",
    "    \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findWithinBraces(text, start):\n",
    "    name = text[start:-3] #[[Category: ]]\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(word):\n",
    "    token_list = word_tokenize(word)\n",
    "    minus_stop_words = remove_stop_words(token_list)\n",
    "    return minus_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer().stem\n",
    "# alphabets = compile(r'\\D+')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemmer(word_list):\n",
    "    global ps\n",
    "    stemmed_list = [ps(word) for word in word_list]\n",
    "    return stemmed_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanhtml(raw_html):\n",
    "    cleanr = compile('<.*?>|\\'*\\[\\[|\\]\\]\\'*|^\\{\\{.*\\}\\}$|\\{\\{.*')\n",
    "    cleantext = sub(cleanr, '', raw_html)\n",
    "    return cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.sax as sx\n",
    "import time\n",
    "\n",
    "class WikiXmlHandler(sx.handler.ContentHandler):\n",
    "    \n",
    "    def __init__(self):\n",
    "        sx.handler.ContentHandler.__init__(self)\n",
    "        self._buffer = None\n",
    "        self._values = {}\n",
    "        self._current_tag = None\n",
    "        self._pages = [] #[(title1, body1), (title2, body2)...]\n",
    "        self._content = {}\n",
    "        self._pageNumber = 0\n",
    "#         self._pageNum_title_map = {}  # {0: Kim Hyeon , 1: Marko Virtanen..}\n",
    "        self._doc_map = {}\n",
    "        self.posting_list = {}\n",
    "        self.categories_list = {}\n",
    "        self.count = {}\n",
    "        \n",
    "    def characters(self, content): # when a character is read\n",
    "        global alphabets\n",
    "        if self._current_tag:\n",
    "#             match = alphabets.match(content)\n",
    "#             if match:\n",
    "            self._buffer.append(sub(r'\\b\\d+(?:\\.\\d+)?\\s+', '', content))\n",
    "\n",
    "    def startElement(self, name, attrs): # when a tag opens\n",
    "        if name == 'page':\n",
    "            self._pageNumber += 1\n",
    "            \n",
    "        if name in ('title', 'text'):\n",
    "            self._current_tag = name\n",
    "            self._buffer = []\n",
    "\n",
    "    def endElement(self, name): # when a tag closes\n",
    "        if name == self._current_tag:\n",
    "            self._values[name] = ' '.join(self._buffer)        \n",
    "\n",
    "        if name == 'page': # when page ends\n",
    "            # now process the text\n",
    "            content = self._values['text']\n",
    "#             self.process_text(content)\n",
    "            self._doc_map[self._pageNumber]  = ({'id':self._pageNumber, 'title': self._values['title'], 'body': content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing:  0.4099061449368795 min ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "parser = sx.make_parser()\n",
    "parser.setFeature(sx.handler.feature_namespaces, 0)\n",
    "handler = WikiXmlHandler()\n",
    "parser.setContentHandler(handler)\n",
    "parser.parse('dataset/enwiki-latest-pages-articles26.xml-p42567204p42663461')\n",
    "print(\"Parsing:  %s min ---\" % ((time.time() - start_time)/60.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text_Preprocessing():\n",
    "    def __init__(self, doc_map):\n",
    "        self.posting_list = {}\n",
    "        self.stop_words = set(stopwords.words('english')).union(['br','\\''])\n",
    "        self.ps = PorterStemmer().stem\n",
    "        self.tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        self.d = doc_map\n",
    "    \n",
    "    def check(self, t1, t2, t3):\n",
    "        \n",
    "        if t1 not in self.posting_list:\n",
    "            self.posting_list[t1] = {}\n",
    "    \n",
    "        if t2 not in self.posting_list[t1]:\n",
    "            self.posting_list[t1][t2] = {}\n",
    "            \n",
    "        if t3 not in self.posting_list[t1][t2]:\n",
    "            self.posting_list[t1][t2][t3] = 0\n",
    "        return self.posting_list\n",
    "    def process_title(self, text, pageNumber):\n",
    "\n",
    "       \n",
    "        token_list = self.tokenizer.tokenize(text)\n",
    "        filtered_sentence = [w for w in token_list if not w in self.stop_words]\n",
    "        stemmed_list = [self.ps(word) for word in filtered_sentence]\n",
    "        #     token_list = tokenize(text)\n",
    "        #     stemmed_list = stemmer(token_list)\n",
    "        for word in stemmed_list:\n",
    "            self.posting_list = self.check(word, pageNumber, 't')\n",
    "            self.posting_list = self.check(word, pageNumber, 'n')\n",
    "            self.posting_list[word][pageNumber]['t'] += 1\n",
    "            self.posting_list[word][pageNumber]['n'] += 1\n",
    "  \n",
    "    def process_categories(self,text, pageNumber):\n",
    "        category_regex = compile(\".*\\[\\[Category:.*\\]\\].*\")\n",
    "        match_cat_list = category_regex.findall(text)\n",
    "\n",
    "        n = len('category') + 4\n",
    "        for one_match in match_cat_list:\n",
    "            text = text.replace(one_match, '')\n",
    "            category_name = one_match[n:-3] # say, Indian Culture\n",
    "            token_list = self.tokenizer.tokenize(category_name)\n",
    "            filtered_sentence = [w for w in token_list if not w in self.stop_words]\n",
    "            stemmed_list = [self.ps(word) for word in filtered_sentence]\n",
    "\n",
    "            for word in stemmed_list: # ['data', 'scienc', 'peopl', 'birth']\n",
    "                self.posting_list = self.check(word, pageNumber, 'c')\n",
    "                self.posting_list = self.check(word, pageNumber, 'n')\n",
    "                self.posting_list[word][pageNumber]['c'] += 1\n",
    "                self.posting_list[word][pageNumber]['n'] += 1\n",
    "        return text\n",
    "    \n",
    "    def process_infobox(self, text, pageNumber):    \n",
    "\n",
    "        infobox_start = compile(\"{{Infobox\")\n",
    "\n",
    "        start_match = search(infobox_start, text)\n",
    "        if start_match:\n",
    "\n",
    "            start_pos = start_match.start()\n",
    "            brack_count = 2\n",
    "            end_pos = start_pos + len(\"{{Infobox \")\n",
    "            while(end_pos < len(text)):\n",
    "                if text[end_pos] == '}':\n",
    "                    brack_count = brack_count - 1\n",
    "                if text[end_pos] == '{':\n",
    "                    brack_count = brack_count + 1\n",
    "                if brack_count == 0:\n",
    "                    break\n",
    "                end_pos = end_pos+1\n",
    "\n",
    "            if end_pos+1 >= len(text):\n",
    "                return\n",
    "            infobox_string = text[start_pos:end_pos+1]  \n",
    "#             print(infobox_string)\n",
    "            text = text.replace(infobox_string, '')\n",
    "            content = infobox_string.split('\\n')\n",
    "\n",
    "            tokens = []\n",
    "            add = tokens.append\n",
    "            heading = content[0][len('{{infobox '):-1]\n",
    "            add(heading)\n",
    "            for idx in range(1,len(content)-2):\n",
    "                try:\n",
    "                    value = \" \".join(findall(r'\\w+', content[idx].split('=',1)[1])).strip()\n",
    "                    add(value)\n",
    "                except:\n",
    "                    pass\n",
    "            tokens = list(filter(lambda x: x.strip(), tokens))\n",
    "            \n",
    "            for one_token in tokens:\n",
    "                token_list = one_token.split()\n",
    "                filtered_sentence = [w for w in token_list if not w in self.stop_words]\n",
    "                print(filtered_sentence)\n",
    "                stemmed_list = [self.ps(word) for word in filtered_sentence]\n",
    "                for word in stemmed_list:\n",
    "#                     print(word)\n",
    "                    self.posting_list = self.check(word, pageNumber, 'i')\n",
    "                    self.posting_list = self.check(word, pageNumber, 'n')\n",
    "                    self.posting_list[word][pageNumber]['i'] += 1\n",
    "                    self.posting_list[word][pageNumber]['n'] += 1\n",
    "        return text\n",
    "\n",
    "    def process_ref(self, text, pageNumber):\n",
    "            pass\n",
    "#             ref_start = compile('< ref.* >(.*?)< /ref >', DOTALL)\n",
    "#             title_start = compile('title')\n",
    "\n",
    "#             sentence = sent_tokenize(text)\n",
    "\n",
    "#             for one_sentence in sentence:\n",
    "#                 match_list = ref_start.findall(one_sentence)\n",
    "#                 if match_list:\n",
    "#                     pipe_tokens = match_list[0].split('|')\n",
    "\n",
    "#                     for one_token in pipe_tokens:\n",
    "#                         if title_start.match(one_token):\n",
    "\n",
    "#                             title = one_token.split('=')[1]\n",
    "\n",
    "#                             token_list = text.split()\n",
    "#                             filtered_sentence = [w for w in token_list if not w in self.stop_words]\n",
    "#                             stemmed_list = [self.ps(word) for word in filtered_sentence]\n",
    "#                             for word in stemmed_list:\n",
    "#                                 self.posting_list = self.check(word, pageNumber, 'r')\n",
    "#                                 self.posting_list = self.check(word, pageNumber, 'n')\n",
    "#                                 self.posting_list[word][pageNumber]['r'] += 1\n",
    "#                                 self.posting_list[word][pageNumber]['n'] += 1\n",
    "    \n",
    "    def process_body_text(self, text, pageNumber):\n",
    "        \n",
    "        body_ = compile('==.*==|\\{\\{.*\\}\\}|#.*|\\{\\{.*|\\|.*|\\}\\}|\\*.*|!.*|\\[\\[|\\]\\]|;.*|&lt;.*&gt;.*&lt;/.*&gt;|<.*>.*</.*>|<.*>')\n",
    "        text = sub(body_, '', text)\n",
    "#         text = text.replace(body_,'')\n",
    "#         print(text)\n",
    "#         print('----------------')\n",
    "        content = text.splitlines()\n",
    "        content = list(filter(lambda x: x.strip(), content))\n",
    "\n",
    "        content = [\" \".join(findall(\"[a-zA-Z]+\", x)).strip() for x in content]\n",
    "        content = list(filter(None, content)) \n",
    "\n",
    "\n",
    "        tokens = content\n",
    "        for one_line in content:\n",
    "               \n",
    "                token_list = text.split()\n",
    "                filtered_sentence = [w for w in token_list if not w in self.stop_words]\n",
    "                stemmed_list = [self.ps(word) for word in filtered_sentence]\n",
    "                for word in stemmed_list:\n",
    "                    self.posting_list = self.check(word, pageNumber, 'b')\n",
    "                    self.posting_list = self.check(word, pageNumber, 'n')\n",
    "                    self.posting_list[word][pageNumber]['b'] += 1\n",
    "                    self.posting_list[word][pageNumber]['n'] += 1\n",
    "        return text\n",
    "    \n",
    "    def make_index(self):\n",
    "        title_regex = compile('.*:')\n",
    "        for k,v in self.d.items():\n",
    "            start_time = time.time()\n",
    "            print('processing no ', k, end = ', ')\n",
    "            match_title = title_regex.match(v['title'])\n",
    "            self.process_title(v['title'], v['id'])\n",
    "            if not match_title:\n",
    "                body = v['body']\n",
    "                x = self.process_categories(body, v['id'])\n",
    "                print('categories done ',end = ', ')\n",
    "                x = self.process_infobox(x, v['id'])\n",
    "                print('infobox done ',end = ', ')\n",
    "                if x is not None:\n",
    "                    self.process_ref(x, v['id'])\n",
    "                print('references done', end = ', ')\n",
    "                if x is not None:\n",
    "                    x = self.process_body_text(x, v['id'])\n",
    "                print('body done ')\n",
    "                a = time.time() - start_time\n",
    "                if a >= 0.007:\n",
    "                    print (a, ' seconds')\n",
    "                    break\n",
    "        return self.posting_list\n",
    "    def make_index_n(self,n):\n",
    "        start= time.time()\n",
    "        print('here')\n",
    "        title_regex = compile('.*:|.*;')\n",
    "        num = n\n",
    "        v = self.d[n]\n",
    "        print('title: ',v['title'])\n",
    "        match_title = title_regex.match(v['title'])\n",
    "        self.process_title(v['title'], num)\n",
    "        print('title done')\n",
    "        if not match_title:\n",
    "            body = v['body']\n",
    "            x = self.process_categories(body, num)\n",
    "            print('categories done ',end = ', ')\n",
    "            x = self.process_infobox(x, num)\n",
    "            print('infobox done ',end = ', ')\n",
    "            if x is not None:\n",
    "                self.process_ref(x, num)\n",
    "            print('references and external links done', end = ', ')\n",
    "            if x is not None:\n",
    "                x = self.process_body_text(x, num)\n",
    "            print('body done ')\n",
    "            print(time.time() - start,' seconds')\n",
    "            \n",
    "        return self.posting_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "title:  2014–Northern Illinois Huskies men's basketball team\n",
      "title done\n",
      "categories done , ['NCAA', 'team', 'season']\n",
      "['Basketball']\n",
      "['2014', '15']\n",
      "['2013', '14']\n",
      "['2015', '16']\n",
      "['Northern', 'Illinois', 'Huskies']\n",
      "['Northern', 'Illinois', 'Huskies', 'basketball', 'wordmark', 'svg']\n",
      "['250']\n",
      "['Mid', 'American', 'Conference']\n",
      "['West', 'Division']\n",
      "['MAC']\n",
      "['14', '16']\n",
      "['8', '10']\n",
      "['Mark', 'Montgomery', 'basketball', 'Mark', 'Montgomery']\n",
      "['4th']\n",
      "['Jon', 'Borovich']\n",
      "['Lou', 'Dawkins']\n",
      "['Jason', 'Larson']\n",
      "['Convocation', 'Center', 'Northern', 'Illinois', 'University', 'Convocation', 'Center']\n",
      "infobox done , references and external links done, body done \n",
      "0.013564348220825195  seconds\n",
      "{'2014': {6: {'t': 1, 'n': 3, 'c': 1, 'i': 1}}, 'northern': {6: {'t': 1, 'n': 8, 'c': 3, 'i': 3, 'b': 1}}, 'illinoi': {6: {'t': 1, 'n': 10, 'c': 4, 'i': 3, 'b': 2}}, 'huski': {6: {'t': 1, 'n': 5, 'c': 1, 'i': 2, 'b': 1}}, 'men': {6: {'t': 1, 'n': 3, 'c': 2}}, 'basketbal': {6: {'t': 1, 'n': 8, 'c': 2, 'i': 3, 'b': 2}}, 'team': {6: {'t': 1, 'n': 2, 'i': 1}}, 'mid': {6: {'c': 1, 'n': 2, 'i': 1}}, 'american': {6: {'c': 1, 'n': 2, 'i': 1}}, 'confer': {6: {'c': 1, 'n': 2, 'i': 1}}, 'season': {6: {'c': 2, 'n': 3, 'i': 1}}, 'sport': {6: {'c': 2, 'n': 2}}, 'norther': {6: {'c': 1, 'n': 1}}, 'ncaa': {6: {'i': 1, 'n': 1}}, '15': {6: {'i': 1, 'n': 1}}, '2013': {6: {'i': 1, 'n': 1}}, '14': {6: {'i': 2, 'n': 2}}, '2015': {6: {'i': 1, 'n': 1}}, '16': {6: {'i': 2, 'n': 2}}, 'wordmark': {6: {'i': 1, 'n': 1}}, 'svg': {6: {'i': 1, 'n': 1}}, '250': {6: {'i': 1, 'n': 1}}, 'west': {6: {'i': 1, 'n': 1}}, 'divis': {6: {'i': 1, 'n': 2, 'b': 1}}, 'mac': {6: {'i': 1, 'n': 1}}, '8': {6: {'i': 1, 'n': 1}}, '10': {6: {'i': 1, 'n': 1}}, 'mark': {6: {'i': 2, 'n': 3, 'b': 1}}, 'montgomeri': {6: {'i': 2, 'n': 3, 'b': 1}}, '4th': {6: {'i': 1, 'n': 1}}, 'jon': {6: {'i': 1, 'n': 1}}, 'borovich': {6: {'i': 1, 'n': 1}}, 'lou': {6: {'i': 1, 'n': 1}}, 'dawkin': {6: {'i': 1, 'n': 1}}, 'jason': {6: {'i': 1, 'n': 1}}, 'larson': {6: {'i': 1, 'n': 1}}, 'convoc': {6: {'i': 2, 'n': 2}}, 'center': {6: {'i': 2, 'n': 2}}, 'univers': {6: {'i': 1, 'n': 2, 'b': 1}}, 'the': {6: {'b': 2, 'n': 2}}, \"'''2014–northern\": {6: {'b': 1, 'n': 1}}, \"men'\": {6: {'b': 2, 'n': 2}}, \"team'''\": {6: {'b': 1, 'n': 1}}, 'repres': {6: {'b': 1, 'n': 1}}, '2014–ncaa': {6: {'b': 1, 'n': 1}}, 'I': {6: {'b': 1, 'n': 1}}, 'season.': {6: {'b': 1, 'n': 1}}, 'huskies,': {6: {'b': 1, 'n': 1}}, 'led': {6: {'b': 1, 'n': 1}}, 'fourth': {6: {'b': 1, 'n': 1}}, 'year': {6: {'b': 1, 'n': 1}}, 'head': {6: {'b': 1, 'n': 1}}, 'coach': {6: {'b': 1, 'n': 1}}, '(basketball)': {6: {'b': 1, 'n': 1}}}\n",
      "--- 0.0002622683842976888 min ---\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "d[1] = {'id':1, 'title':Abc, 'body': xjse}\n",
    "'''\n",
    "\n",
    "start_time2 = time.time()\n",
    "d = handler._doc_map\n",
    "indexer = Text_Preprocessing(d)\n",
    "# i = indexer.make_index()\n",
    "i = indexer.make_index_n(6)\n",
    "print(i)\n",
    "print(\"--- %s min ---\" % ((time.time() - start_time2)/60.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_titles(keyword):\n",
    "    global d\n",
    "    doc_ids_dict = posting_list[keyword]\n",
    "    for key,val in doc_ids_dict.items():\n",
    "        print(d[key]['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 7231\n",
    "title = d[n]['title']\n",
    "print(title)\n",
    "body = d[n]['body']\n",
    "x = body\n",
    "# match_title = title_regex.match(d[n]['title'])\n",
    "# if not match_title:\n",
    "    \n",
    "#     process_title(d[n]['title'], n)\n",
    "    \n",
    "\n",
    "#     x = process_categories(body, num)\n",
    "#     print('categories done ',end = ', ')\n",
    "#     x = process_infobox(x, num)\n",
    "#     print('infobox done ',end = ', ')\n",
    "x = process_ref(x, num)\n",
    "#     if x is not None:\n",
    "#         x = process_body_text(x, num)\n",
    "#     print('body done ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(find_titles('napier'))\n",
    "n = 20\n",
    "print (len(handler._doc_map), len(posting_list))\n",
    "mydict = posting_list\n",
    "firstnpairs = {k: mydict[k] for k in list(mydict)[:n]}\n",
    "# print (firstnpairs)\n",
    "# print(mydict['ice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('myfile.txt', 'w') as f:\n",
    "    print(posting_list, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(mydict, orient='index').stack().reset_index()\n",
    "df.columns = ['id','field','content']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
