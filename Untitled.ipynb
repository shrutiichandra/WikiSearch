{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'enwiki-latest-pages-articles26.xml-p42567204p42663461'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import search, match, findall, sub, compile, finditer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(word_tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findWithinBraces(text, start):\n",
    "    name = text[start:-3] #[[Category: ]]\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(word):\n",
    "    token_list = word_tokenize(word)\n",
    "    minus_stop_words = remove_stop_words(token_list)\n",
    "    return minus_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer().stem\n",
    "alphabets = compile(r'\\D+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemmer(word_list):\n",
    "    global ps\n",
    "    stemmed_list = [ps(word) for word in word_list]\n",
    "    return stemmed_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.sax as sx\n",
    "import time\n",
    "\n",
    "class WikiXmlHandler(sx.handler.ContentHandler):\n",
    "    \n",
    "    def __init__(self):\n",
    "        sx.handler.ContentHandler.__init__(self)\n",
    "        self._buffer = None\n",
    "        self._values = {}\n",
    "        self._current_tag = None\n",
    "        self._pages = [] #[(title1, body1), (title2, body2)...]\n",
    "        self._content = {}\n",
    "        self._pageNumber = 0\n",
    "#         self._pageNum_title_map = {}  # {0: Kim Hyeon , 1: Marko Virtanen..}\n",
    "        self._doc_map = {}\n",
    "        self.posting_list = {}\n",
    "        self.categories_list = {}\n",
    "        self.count = {}\n",
    "        \n",
    "    def characters(self, content): # when a character is read\n",
    "        global alphabets\n",
    "        if self._current_tag:\n",
    "#             match = alphabets.match(content)\n",
    "#             if match:\n",
    "            self._buffer.append(content)\n",
    "\n",
    "    def startElement(self, name, attrs): # when a tag opens\n",
    "        if name == 'page':\n",
    "            self._pageNumber += 1\n",
    "            \n",
    "        if name in ('title', 'text'):\n",
    "            self._current_tag = name\n",
    "            self._buffer = []\n",
    "\n",
    "    def endElement(self, name): # when a tag closes\n",
    "        if name == self._current_tag:\n",
    "            self._values[name] = ' '.join(self._buffer)        \n",
    "\n",
    "        if name == 'page': # when page ends\n",
    "            # now process the text\n",
    "            content = self._values['text']\n",
    "#             self.process_text(content)\n",
    "            self._doc_map[self._pageNumber]  = ({'id':self._pageNumber, 'title': self._values['title'], 'body': content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing:  0.08587478001912435 min ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "parser = sx.make_parser()\n",
    "parser.setFeature(sx.handler.feature_namespaces, 0)\n",
    "handler = WikiXmlHandler()\n",
    "parser.setContentHandler(handler)\n",
    "parser.parse('dataset/enwiki-latest-pages-articles26.xml-p42567204p42663461')\n",
    "print(\"Parsing:  %s min ---\" % ((time.time() - start_time)/60.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = {} \n",
    "categories_list = {}\n",
    "posting_list = {}\n",
    "i = 0\n",
    "def process_text(text, pageNumber):\n",
    "        global i\n",
    "        category_regex = compile(\".*\\[\\[Category:.*\\]\\].*\")\n",
    "        match_cat_list = category_regex.findall(text)\n",
    "        key = str(pageNumber) + 'c'\n",
    "        n = len('category') + 4\n",
    "        for one_match in match_cat_list:\n",
    "            text = text.replace(one_match, '')\n",
    "            categories = findWithinBraces(one_match, n) # say, Indian Culture\n",
    "            try:\n",
    "                count[categories] += 1\n",
    "            except:\n",
    "                count[categories] = 1\n",
    "            \n",
    "            if count[categories] == 1:\n",
    "                #tokenize and remove stop words\n",
    "                token_list = tokenize(categories)\n",
    "                 #do stemming\n",
    "                stemmed_list = stemmer(token_list)\n",
    "                categories_list[categories] = [x for x in stemmed_list]\n",
    "\n",
    "            for word in categories_list[categories]:\n",
    "                try:\n",
    "                    if key not in posting_list[word]:\n",
    "                        posting_list[word].append(key)\n",
    "                except:\n",
    "                    posting_list[word] = [key]\n",
    "        return text\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanhtml(raw_html):\n",
    "    cleanr = compile('<.*?>|\\'*\\[\\[|\\]\\]\\'*|^\\{\\{.*\\}\\}$|\\{\\{.*')\n",
    "    cleantext = sub(cleanr, '', raw_html)\n",
    "    return cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "infobox = {}\n",
    "i = 0\n",
    "def getInfoBox(text, pageNum):    \n",
    "    global alphabets\n",
    "    global i\n",
    "    infobox_start = compile(\"{{Infobox\")\n",
    "    key = str(pageNum) + 'i'\n",
    "    \n",
    "    start_match = search(infobox_start, text)\n",
    "    if start_match:\n",
    "       \n",
    "        start_pos = start_match.start()\n",
    "        brack_count = 2\n",
    "        end_pos = start_pos + len(\"{{Infobox \")\n",
    "        while(end_pos < len(text)):\n",
    "            if text[end_pos] == '}':\n",
    "                brack_count = brack_count - 1\n",
    "            if text[end_pos] == '{':\n",
    "                brack_count = brack_count + 1\n",
    "            if brack_count == 0:\n",
    "                break\n",
    "            end_pos = end_pos+1\n",
    "            \n",
    "        if end_pos+1 >= len(text):\n",
    "            return\n",
    "        infobox_string = text[start_pos:end_pos+1]  \n",
    "        text = text.replace(infobox_string, '')\n",
    "        \n",
    "        \n",
    "#         text = sub(infobox_string, text, '')\n",
    "#         if i<=3:\n",
    "#             print (text)\n",
    "        content = infobox_string.split('\\n')\n",
    "     \n",
    "        for one_line in content:\n",
    "            tokens = []\n",
    "            add = tokens.append\n",
    "            add(content[0][len('{{infobox '):-1])\n",
    "            for idx in range(1,len(content)-2):\n",
    "\n",
    "                #split at =    \n",
    "                if '=' in content[idx]:\n",
    "                    value = alphabets.match(cleanhtml(content[idx].split('=',1)[1].strip()))\n",
    "                    if value:\n",
    "                        add(value.group())                        \n",
    "            for one_token in tokens:\n",
    "                try:\n",
    "                    count[one_token] += 1\n",
    "                except:\n",
    "                    count[one_token] = 1\n",
    "\n",
    "                if count[one_token] == 1:\n",
    "                    #tokenize and remove stop words\n",
    "                    token_list = tokenize(one_token)\n",
    "                     #do stemming\n",
    "                    stemmed_list = stemmer(token_list)\n",
    "                    categories_list[one_token] = [x for x in stemmed_list]\n",
    "\n",
    "                for word in categories_list[one_token]:\n",
    "                    try:\n",
    "                        if key not in posting_list[word]:\n",
    "                            posting_list[word].append(key)\n",
    "                    except:\n",
    "                        posting_list[word] = [key]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_body_text(text, pageNum):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "dict[1] = {'id':1, 'title':Abc, 'body': xjse}\n",
    "'''\n",
    "i = 0\n",
    "start_time = time.time()\n",
    "d = handler._doc_map\n",
    "for k,v in d.items():\n",
    "    i+=1\n",
    "   \n",
    "    body = v['body']\n",
    "    num = v['id']\n",
    "    x = process_text(body, num)\n",
    "    x = getInfoBox(x, num)\n",
    "    x = process_bod_text()\n",
    "\n",
    "print(\"--- %s min ---\" % ((time.time() - start_time)/60.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20\n",
    "print (len(handler._doc_map), len(posting_list))\n",
    "mydict = posting_list\n",
    "firstnpairs = {k: mydict[k] for k in list(mydict)[:n]}\n",
    "# print (firstnpairs)\n",
    "print(mydict['ice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(mydict, orient='index').stack().reset_index()\n",
    "df.columns = ['id','field','content']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
