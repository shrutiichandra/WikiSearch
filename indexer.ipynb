{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'enwiki-latest-pages-articles26.xml-p42567204p42663461'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.stem import PorterStemmer \n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import ToktokTokenizer\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import search, match, findall, sub, compile, finditer, DOTALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.sax as sx\n",
    "import time\n",
    "\n",
    "class WikiXmlHandler(sx.handler.ContentHandler):\n",
    "    \n",
    "    def __init__(self):\n",
    "        sx.handler.ContentHandler.__init__(self)\n",
    "        self._buffer = None\n",
    "        self._values = {}\n",
    "        self._current_tag = None\n",
    "        self._pages = [] #[(title1, body1), (title2, body2)...]\n",
    "        self._content = {}\n",
    "        self._pageNumber = 0\n",
    "#         self._pageNum_title_map = {}  # {0: Kim Hyeon , 1: Marko Virtanen..}\n",
    "        self._doc_map = {}\n",
    "        self.posting_list = {}\n",
    "        self.categories_list = {}\n",
    "        self.count = {}\n",
    "        \n",
    "    def characters(self, content): # when a character is read\n",
    "        global alphabets\n",
    "        if self._current_tag:\n",
    "#             match = alphabets.match(content)\n",
    "#             if match:\n",
    "            self._buffer.append(sub(r'\\b\\d+(?:\\.\\d+)?\\s+', '', content))\n",
    "\n",
    "    def startElement(self, name, attrs): # when a tag opens\n",
    "        if name == 'page':\n",
    "            self._pageNumber += 1\n",
    "            \n",
    "        if name in ('title', 'text'):\n",
    "            self._current_tag = name\n",
    "            self._buffer = []\n",
    "\n",
    "    def endElement(self, name): # when a tag closes\n",
    "        if name == self._current_tag:\n",
    "            self._values[name] = ' '.join(self._buffer)        \n",
    "\n",
    "        if name == 'page': # when page ends\n",
    "            # now process the text\n",
    "            content = self._values['text']\n",
    "#             self.process_text(content)\n",
    "            self._doc_map[self._pageNumber]  = ({'id':self._pageNumber, 'title': self._values['title'], 'body': content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing:  0.4099061449368795 min ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "parser = sx.make_parser()\n",
    "parser.setFeature(sx.handler.feature_namespaces, 0)\n",
    "handler = WikiXmlHandler()\n",
    "parser.setContentHandler(handler)\n",
    "parser.parse('dataset/enwiki-latest-pages-articles26.xml-p42567204p42663461')\n",
    "print(\"Parsing:  %s min ---\" % ((time.time() - start_time)/60.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text_Preprocessing():\n",
    "    def __init__(self, doc_map):\n",
    "        self.posting_list = {}\n",
    "        self.mine = ['br','\\'','http','url','web','www','blp','ref','external','links']\n",
    "        self.stop_words = set(stopwords.words('english')).union(self.mine)\n",
    "        self.ps = PorterStemmer().stem\n",
    "        self.tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        self.d = doc_map\n",
    "        self.t = 0\n",
    "        self.toktok = ToktokTokenizer()\n",
    "    \n",
    "    def check(self, t1, t2, t3):\n",
    "        \n",
    "        if t1 not in self.posting_list:\n",
    "            self.posting_list[t1] = {}\n",
    "    \n",
    "        if t2 not in self.posting_list[t1]:\n",
    "            self.posting_list[t1][t2] = {}\n",
    "            \n",
    "        if t3 not in self.posting_list[t1][t2]:\n",
    "            self.posting_list[t1][t2][t3] = 0\n",
    "        return self.posting_list\n",
    "    def process_title(self, text, pageNumber):\n",
    "\n",
    "        text = text.lower()\n",
    "        token_list = self.tokenizer.tokenize(text)\n",
    "        filtered_sentence = [w for w in token_list if not w in self.stop_words]\n",
    "        stemmed_list = [self.ps(word) for word in filtered_sentence]\n",
    "        #     token_list = tokenize(text)\n",
    "        #     stemmed_list = stemmer(token_list)\n",
    "#         print('title: ',filtered_sentence)\n",
    "        for word in stemmed_list:\n",
    "            self.posting_list = self.check(word, pageNumber, 't')\n",
    "            self.posting_list = self.check(word, pageNumber, 'n')\n",
    "            self.posting_list[word][pageNumber]['t'] += 1\n",
    "            self.posting_list[word][pageNumber]['n'] += 1\n",
    "  \n",
    "    def process_categories(self,text, pageNumber):\n",
    "        category_regex = compile(\".*\\[\\[Category:.*\\]\\].*\")\n",
    "        match_cat_list = category_regex.findall(text)\n",
    "        total_stems = []\n",
    "        n = len('category') + 4\n",
    "        total_stems = []\n",
    "        extend = total_stems.extend\n",
    "        for one_match in match_cat_list:\n",
    "            text = text.replace(one_match, '')\n",
    "            category_name = one_match[n:-3] # say, Indian Culture\n",
    "            category_name = category_name.lower()\n",
    "            token_list = self.tokenizer.tokenize(category_name)\n",
    "            filtered_sentence = [w for w in token_list if not w in self.stop_words]\n",
    "            stemmed_list = [self.ps(word) for word in filtered_sentence]\n",
    "            extend(stemmed_list)\n",
    "        \n",
    "        for word in total_stems: # ['data', 'scienc', 'peopl', 'birth']\n",
    "            self.posting_list = self.check(word, pageNumber, 'c')\n",
    "            self.posting_list = self.check(word, pageNumber, 'n')\n",
    "            self.posting_list[word][pageNumber]['c'] += 1\n",
    "            self.posting_list[word][pageNumber]['n'] += 1\n",
    "        return text\n",
    "    \n",
    "    def process_infobox(self, text, pageNumber):    \n",
    "\n",
    "        infobox_start = compile(\"{{Infobox\")\n",
    "\n",
    "        start_match = search(infobox_start, text)\n",
    "        if start_match:\n",
    "\n",
    "            start_pos = start_match.start()\n",
    "            brack_count = 2\n",
    "            end_pos = start_pos + len(\"{{Infobox \")\n",
    "            while(end_pos < len(text)):\n",
    "                if text[end_pos] == '}':\n",
    "                    brack_count = brack_count - 1\n",
    "                if text[end_pos] == '{':\n",
    "                    brack_count = brack_count + 1\n",
    "                if brack_count == 0:\n",
    "                    break\n",
    "                end_pos = end_pos+1\n",
    "\n",
    "            if end_pos+1 >= len(text):\n",
    "                return\n",
    "            infobox_string = text[start_pos:end_pos+1]  \n",
    "#             print(infobox_string)\n",
    "            text = text.replace(infobox_string, '')\n",
    "            content = infobox_string.split('\\n')\n",
    "            content = list(map(lambda x:x.lower(),content))\n",
    "            tokens = []\n",
    "            add = tokens.append\n",
    "            heading = content[0][len('{{infobox '):-1]\n",
    "            add(heading)\n",
    "            for idx in range(1,len(content)-2):\n",
    "                try:\n",
    "                    value = \" \".join(findall(r'\\w+', content[idx].split('=',1)[1])).strip()\n",
    "                    add(value)\n",
    "                except:\n",
    "                    pass\n",
    "            tokens = list(filter(lambda x: x.strip(), tokens))\n",
    "            total_stems = []\n",
    "            extend = total_stems.extend\n",
    "            for one_token in tokens:\n",
    "                token_list = one_token.split()\n",
    "                filtered_sentence = [w for w in token_list if not w in self.stop_words]\n",
    "#                 print('infobox: ',filtered_sentence)\n",
    "                stemmed_list = [self.ps(word) for word in filtered_sentence]\n",
    "                extend(stemmed_list)\n",
    "            for word in total_stems:\n",
    "#                     print(word)\n",
    "                self.posting_list = self.check(word, pageNumber, 'i')\n",
    "                self.posting_list = self.check(word, pageNumber, 'n')\n",
    "                self.posting_list[word][pageNumber]['i'] += 1\n",
    "                self.posting_list[word][pageNumber]['n'] += 1\n",
    "        return text\n",
    "\n",
    "    def process_ref(self, text, pageNumber):\n",
    "#             pass\n",
    "            ref_start = compile('< ref.* >(.*?)< /ref >', DOTALL)\n",
    "            title_start = compile('.*title =|.*title=')\n",
    "\n",
    "            tokenized_corpus = [ref_start.findall(sent) for sent in sent_tokenize(text) if len(ref_start.findall(sent))>0  ]\n",
    "            tokenized_corpus = list(chain(*tokenized_corpus))\n",
    "#             print(len(sentence))\n",
    "#             print('------------')\n",
    "            if len(tokenized_corpus) > 4:\n",
    "                tokenized_corpus = tokenized_corpus[0:4]\n",
    "#             print(tokenized_corpus)\n",
    "            total_stems = []\n",
    "            extend = total_stems.extend\n",
    "            for match_list in tokenized_corpus:\n",
    "#                 match_list = ref_start.findall(one_sentence)\n",
    "#                 print(match_list)\n",
    "                text = text.replace(match_list, '')\n",
    "                pipe_tokens = match_list.split('|')\n",
    "                for one_token in pipe_tokens:\n",
    "\n",
    "                    if title_start.match(one_token):\n",
    "\n",
    "                        title = one_token.split('=')[1]\n",
    "#                             print(title)\n",
    "\n",
    "                        token_list = title.split()\n",
    "                        filtered_sentence = [w.lower() for w in token_list if not w in self.stop_words]\n",
    "                        stemmed_list = [self.ps(word) for word in filtered_sentence]\n",
    "                        extend(stemmed_list)\n",
    "            \n",
    "            for word in total_stems:\n",
    "                self.posting_list = self.check(word, pageNumber, 'r')\n",
    "                self.posting_list = self.check(word, pageNumber, 'n')\n",
    "                self.posting_list[word][pageNumber]['r'] += 1\n",
    "                self.posting_list[word][pageNumber]['n'] += 1\n",
    "    \n",
    "    def process_body_text(self, text, pageNumber):\n",
    "        \n",
    "        body_ = compile('==.*==|\\{\\{.*\\}\\}|#.*|\\{\\{.*|\\|.*|\\}\\}|\\*.*|!.*|\\[\\[|\\]\\]|;.*|&lt;.*&gt;.*&lt;/.*&gt;|<.*>.*</.*>|<.*>')\n",
    "        matches = body_.findall(text)\n",
    "        text = str(filter(lambda x: text.replace(x,''), matches ))\n",
    "#         text = sub(body_, '', text)\n",
    "#         text = text.replace(body_,'')\n",
    "#         print(text)\n",
    "#         print('----------------')\n",
    "        content = text.splitlines()\n",
    "        content = list(filter(lambda x: x.strip(), content))\n",
    "#         content = self.tokenizer.tokenize(text)\n",
    "        content = [\" \".join(findall(\"[a-zA-Z]+\", x)).strip() for x in content]\n",
    "        content = list(filter(None, content)) \n",
    "        \n",
    "        content = list(map(lambda x:x.lower(),content))\n",
    "        \n",
    "        total_stems = []\n",
    "        extend = total_stems.extend\n",
    "        for one_line in content:\n",
    "               \n",
    "            token_list = word_tokenize(one_line)\n",
    "            filtered_sentence = [w for w in token_list if not w in self.stop_words]\n",
    "            #                 print('body: ',filtered_sentence)\n",
    "            stemmed_list = [self.ps(word) for word in filtered_sentence]\n",
    "            extend(stemmed_list)\n",
    "#         print(total_stems)\n",
    "        for word in total_stems:\n",
    "            self.posting_list = self.check(word, pageNumber, 'b')\n",
    "            self.posting_list = self.check(word, pageNumber, 'n')\n",
    "            self.posting_list[word][pageNumber]['b'] += 1\n",
    "            self.posting_list[word][pageNumber]['n'] += 1\n",
    "        return text\n",
    "    \n",
    "    def make_index(self):\n",
    "        title_regex = compile('.*:')\n",
    "        for k,v in self.d.items():\n",
    "            start_time = time.time()\n",
    "            print('processing no ', k, end = ', ')\n",
    "            match_title = title_regex.match(v['title'])\n",
    "            self.process_title(v['title'], v['id'])\n",
    "            if not match_title:\n",
    "                body = v['body']\n",
    "                x = self.process_categories(body, v['id'])\n",
    "#                 print('categories done ',end = ', ')\n",
    "                x = self.process_infobox(x, v['id'])\n",
    "#                 print('infobox done ',end = ', ')\n",
    "                if x is not None:\n",
    "                    self.process_ref(x, v['id'])\n",
    "#                 print('references done', end = ', ')\n",
    "                if x is not None:\n",
    "                    x = self.process_body_text(x, v['id'])\n",
    "#                 print('body done ')\n",
    "                a = time.time() - start_time\n",
    "                if a >= 0.02:\n",
    "                    self.t += a\n",
    "                    \n",
    "        return self.posting_list\n",
    "    def make_index_n(self,n):\n",
    "        start= time.time()\n",
    "#         print('here')\n",
    "        title_regex = compile('.*:|.*;')\n",
    "        num = n\n",
    "        v = self.d[n]\n",
    "#         print('title: ',v['title'])\n",
    "        match_title = title_regex.match(v['title'])\n",
    "        self.process_title(v['title'], num)\n",
    "        print('title done')\n",
    "        if not match_title:\n",
    "            body = v['body']\n",
    "            x = self.process_categories(body, num)\n",
    "            print('categories done ',end = ', ')\n",
    "            x = self.process_infobox(x, num)\n",
    "            print('infobox done ',end = ', ')\n",
    "            if x is not None:\n",
    "                self.process_ref(x, num)\n",
    "            print('references and external links done', end = ', ')\n",
    "            if x is not None:\n",
    "                x = self.process_body_text(x, num)\n",
    "            print('body done ')\n",
    "        print(time.time() - start,' seconds')\n",
    "            \n",
    "        return self.posting_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "d[1] = {'id':1, 'title':Abc, 'body': xjse}\n",
    "'''\n",
    "\n",
    "start_time2 = time.time()\n",
    "d = handler._doc_map\n",
    "indexer = Text_Preprocessing(d)\n",
    "i = indexer.make_index()\n",
    "# i = indexer.make_index_n(3)\n",
    "print(\"--- %s min ---\" % ((time.time() - start_time2)/60.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59891\n"
     ]
    }
   ],
   "source": [
    "print(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_titles(keyword):\n",
    "    global d\n",
    "    global i\n",
    "    doc_ids_dict = i[keyword]\n",
    "    for key,val in doc_ids_dict.items():\n",
    "        print(d[key]['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 7231\n",
    "title = d[n]['title']\n",
    "print(title)\n",
    "body = d[n]['body']\n",
    "x = body\n",
    "# match_title = title_regex.match(d[n]['title'])\n",
    "# if not match_title:\n",
    "    \n",
    "#     process_title(d[n]['title'], n)\n",
    "    \n",
    "\n",
    "#     x = process_categories(body, num)\n",
    "#     print('categories done ',end = ', ')\n",
    "#     x = process_infobox(x, num)\n",
    "#     print('infobox done ',end = ', ')\n",
    "x = process_ref(x, num)\n",
    "#     if x is not None:\n",
    "#         x = process_body_text(x, num)\n",
    "#     print('body done ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('myfile.txt', 'w') as f:\n",
    "    print(i, file=f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
