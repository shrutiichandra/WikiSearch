{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_dict_folder = './'\n",
    "dict_file_name = 'myfile.txt'\n",
    "path = path_to_dict_folder + dict_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict    \n",
    "from collections import OrderedDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "index = {}\n",
    "with open(path, 'r') as f:\n",
    "    s = f.read()\n",
    "    index = ast.literal_eval(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_map = {}\n",
    "with open(path_to_dict_folder + 'mapping.txt', 'r') as f:\n",
    "    s = f.read()\n",
    "    doc_map = ast.literal_eval(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Query_PreProcess():\n",
    "    def __init__(self,q, index, doc_map):\n",
    "        self.q = q\n",
    "        self.mine = ['br','\\'','http','url','web','www','blp','ref','external','links']\n",
    "        self.stop_words = set(stopwords.words('english')).union(self.mine)\n",
    "        self.ps = PorterStemmer().stem\n",
    "        self.tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        self.fields = {'title':'t', 'body':'b', 'category':'c', 'infobox':'i', 'ref':'r'}\n",
    "        self.fquery_dict = {}\n",
    "        self.nquery = []\n",
    "        self.index = index\n",
    "        self.results = {}\n",
    "        self.doc_map = doc_map\n",
    "    def check_if_field_query(self):\n",
    "        \n",
    "        space_split = self.q.split()\n",
    "        \n",
    "#                 print('infobox: ',filtered_sentence)\n",
    "        space_split = list(map(lambda x:x.lower(),space_split))\n",
    "                \n",
    "        for terms in space_split:\n",
    "            colon_split = terms.split(':')\n",
    "            if colon_split[0] in self.fields:\n",
    "                self.fquery_dict[self.fields[colon_split[0]]] = colon_split[1]\n",
    "                space_split.remove(terms)\n",
    "            else:\n",
    "                pass\n",
    "                # query like 4:50 ; not a field query\n",
    "        filtered_sentence = [w for w in space_split if not w in self.stop_words]\n",
    "        stemmed_list = [self.ps(word) for word in filtered_sentence]\n",
    "        self.nquery = stemmed_list\n",
    "#         print(self.nquery)\n",
    "    \n",
    "    def retrieve_pages(self):\n",
    "        list_of_list = []\n",
    "        add = list_of_list.append\n",
    "        if not self.fquery_dict:\n",
    "            for term in self.nquery:\n",
    "                self.results[term] = {}\n",
    "                dict_of_docs = index[term] # { docid: {'t':2, 'n':3}, docid2: {'c':3,'n':3} } \n",
    "    #             print(type(dict_of_docs))\n",
    "                for k,v in dict_of_docs.items():\n",
    "    #                 print('here')\n",
    "                    this_doc_dict = dict_of_docs[k] #{'t':2, 'n':3}\n",
    "    #                 print(this_doc_dict)\n",
    "                    self.results[term][k] = this_doc_dict['n']\n",
    "                add(sorted(self.results[term].items(), key=lambda item: item[1], reverse=True)[:10])\n",
    "            all_tuples = list(chain(*list_of_list))\n",
    "\n",
    "            d = OrderedDict()\n",
    "\n",
    "            for a, *b in all_tuples:\n",
    "                if a in d:\n",
    "                     d[a] = d[a] + b[0]\n",
    "                else:\n",
    "                     d[a] = b[0]\n",
    "            common_docs = set()\n",
    "            ins = common_docs.add\n",
    "            found_top_ten = False\n",
    "            for k,v in d.items():\n",
    "                if len(common_docs) >= 10:\n",
    "                    found_top_ten = True\n",
    "                    break\n",
    "                ins(k)\n",
    "\n",
    "\n",
    "            print(common_docs)\n",
    "            for i in common_docs:\n",
    "                self.find_titles(i)\n",
    "\n",
    "    def search(self):\n",
    "        self.check_if_field_query()\n",
    "        self.retrieve_pages()\n",
    "#         print(self.results['york'])\n",
    "\n",
    "    def find_titles(self, id_no):\n",
    "        print(self.doc_map[id_no]['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{21665, 24193, 23397, 28031, 27674, 2940, 21663}\n",
      "F.H. Napier\n",
      "St. Catherine of Siena Academy\n",
      "You Scare Me to Death\n",
      "Wendy Napier\n",
      "Mervyn Napier Waller\n",
      "Rupert Worker\n",
      "F. H. Napier\n"
     ]
    }
   ],
   "source": [
    "query = \"napier\"\n",
    "q = Query_PreProcess(query, index, doc_map)\n",
    "q.search()\n",
    "# print(q.nquery)\n",
    "# print(q.fquery_dict)\n",
    "# print(type(q))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
