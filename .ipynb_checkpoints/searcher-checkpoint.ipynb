{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from itertools import chain\n",
    "\n",
    "from collections import defaultdict    \n",
    "from collections import OrderedDict\n",
    "\n",
    "import ast\n",
    "doc_map = {}\n",
    "with open('../res/mapping.txt', 'r') as f:\n",
    "    s = f.read()\n",
    "    doc_map = ast.literal_eval(s)\n",
    "\n",
    "def read_index(path):\n",
    "    index = {}\n",
    "    with open(path, 'r') as f:\n",
    "        s = f.read()\n",
    "        index = ast.literal_eval(s)\n",
    "    return index\n",
    "\n",
    "# Assumes either pure field queries , with fields of only one term; or pure non field query; \n",
    "#NOT A MIXTURE OF FIELD AND NON FIELD\n",
    "class Query_PreProcess():\n",
    "    def __init__(self,q, index):\n",
    "        self.q = q\n",
    "        self.mine = ['br','\\'','http','url','web','www','blp','ref','external','links']\n",
    "        self.stop_words = set(stopwords.words('english')).union(self.mine)\n",
    "        self.ps = PorterStemmer().stem\n",
    "        self.tokenizer = RegexpTokenizer(r'\\w+')\n",
    "        self.fields = {'title':'t', 'body':'b', 'category':'c', 'infobox':'i', 'ref':'r'}\n",
    "        self.fquery_dict = {}\n",
    "        self.nquery = []\n",
    "        self.results = {}\n",
    "        self.outputs = []\n",
    "        \n",
    "        self.index = index\n",
    "\n",
    "    \n",
    "    def check_if_field_query(self):\n",
    "#         print('processing ', self.q)\n",
    "        space_split = self.q.split()\n",
    "        \n",
    "        \n",
    "\n",
    "        space_split = list(map(lambda x:x.lower(),space_split))\n",
    "#         print(space_split)\n",
    "        \n",
    "        while space_split:\n",
    "            terms = space_split[0]\n",
    "            \n",
    "            colon_split = terms.split(':') #if no colon\n",
    "#             print(colon_split)\n",
    "            if colon_split[0] in self.fields:\n",
    "                self.fquery_dict[self.fields[colon_split[0]]] = colon_split[1] #{t:new, c:old}\n",
    "                space_split.remove(terms)\n",
    "            else:\n",
    "                break\n",
    "                # query like 4:50 ; not a field query, do nothing and break\n",
    "        filtered_sentence = [w for w in space_split if not w in self.stop_words]\n",
    "        stemmed_list = [self.ps(word) for word in filtered_sentence]\n",
    "        self.nquery = stemmed_list\n",
    "#         print(self.nquery)\n",
    "    \n",
    "    def retrieve_pages(self):\n",
    "        # for each query term, list_of_list has a list, in which each element is a tuple. \n",
    "        #first index doc id, second frequency in that doc\n",
    "        # say, query is 'hyd patna'\n",
    "        # list_of_list = [[(9, 9), (10, 2)...], [(3,2), (15, 4)...]]\n",
    "        list_of_list = []\n",
    "        add = list_of_list.append\n",
    "        common_docs = set()\n",
    "        ins = common_docs.add\n",
    "        phi = True\n",
    "#         print('nquery: ',self.nquery)\n",
    "        if not self.fquery_dict:\n",
    "#             print('not a field query')\n",
    "            for term in self.nquery:\n",
    "#                 print('term: ',term)\n",
    "                self.results[term] = {}\n",
    "                dict_of_docs = self.index[term] # { docid: {'t':2, 'n':3}, docid2: {'c':3,'n':3} } \n",
    "                for k,v in dict_of_docs.items():\n",
    "                    this_doc_dict = dict_of_docs[k] #{'t':2, 'n':3}\n",
    "                    self.results[term][k] = this_doc_dict['n']\n",
    "                add(sorted(self.results[term].items(), key=lambda item: item[1], reverse=True)[:10])\n",
    "        \n",
    "        else: # if field query\n",
    "            occurence = []\n",
    "            add_o = occurence.append\n",
    "            for field, keyword in self.fquery_dict.items():\n",
    "                doc_ids = []\n",
    "                add_d = doc_ids.append\n",
    "                if len(keyword.split())==1:\n",
    "                    self.results[keyword] = {}\n",
    "                    dict_of_docs = self.index[keyword] # { docid: {'t':2, 'n':3}, docid2: {'c':3,'n':3} } \n",
    "        \n",
    "                    for docId,map_ in dict_of_docs.items():\n",
    "                        this_doc_dict = dict_of_docs[docId] #{'t':2, 'n':3}\n",
    "                        if field in this_doc_dict:\n",
    "                            self.results[keyword][docId] = this_doc_dict[field]\n",
    "                            add_d(docId)\n",
    "                        \n",
    "                    add(sorted(self.results[keyword].items(), key=lambda item: item[1], reverse=True)[:10])\n",
    "                    add_o(doc_ids)\n",
    "            list_of_list = list(filter(None, list_of_list))\n",
    "            occurence = list(filter(None, occurence))\n",
    "            \n",
    "#             print(list_of_list)\n",
    "\n",
    "            \n",
    "            common_docs = set(occurence[0]).intersection(*occurence)\n",
    "            \n",
    "            \n",
    "            if common_docs:\n",
    "                phi = False\n",
    "                # no need to do anything\n",
    "                \n",
    "        if phi:\n",
    "            all_tuples = list(chain(*list_of_list))\n",
    "            d = OrderedDict()\n",
    "\n",
    "            for a, *b in all_tuples:\n",
    "                if a in d:\n",
    "                     d[a] = d[a] + b[0]\n",
    "                else:\n",
    "                     d[a] = b[0]\n",
    "            \n",
    "        \n",
    "            found_top_ten = False\n",
    "            for k,v in d.items():\n",
    "                if len(common_docs) >= 10:\n",
    "                    found_top_ten = True\n",
    "                    break\n",
    "                ins(k)\n",
    "\n",
    "            if not found_top_ten:\n",
    "                pass\n",
    "\n",
    "        # do this for both field, non field query\n",
    "        for i in common_docs:\n",
    "            self.find_titles(i)\n",
    "        \n",
    "    def search(self):\n",
    "        self.check_if_field_query()\n",
    "        self.retrieve_pages()\n",
    "        return self.outputs\n",
    "\n",
    "    def find_titles(self, id_no):\n",
    "        self.outputs.append(doc_map[id_no]['title'])\n",
    "\n",
    "# index_file = '../res/index/index.txt'\n",
    "# index = read_index(index_file)\n",
    "\n",
    "# query = \"arjun\"\n",
    "# q = Query_PreProcess(query, index)\n",
    "# o = q.search()\n",
    "# print(o)\n",
    "# # print(q.nquery)\n",
    "# # print(q.fquery_dict)\n",
    "# # print(type(q))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
